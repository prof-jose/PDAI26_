{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM examples\n",
    "\n",
    "Learning goals: \n",
    "\n",
    "- Practice different introductory use cases of LLMs\n",
    "\n",
    "## Setup\n",
    "\n",
    "- To run this notebook you will need to create an account in cohere.ai, and get your API Key.\n",
    "- You also need to install the python package `cohere`.\n",
    "\n",
    "## Why cohere? \n",
    "\n",
    "- It was one of the first providers of LLMs as a service, was already in the market before ChatGPT. \n",
    "\n",
    "- Easy to use Python SDK. \n",
    "\n",
    "- A free account gives you access to a Trial API key, which allows 5 calls per minute. \n",
    "\n",
    "- Similar in structure to other APIs and frameworks (if you learn this, you'll navigate the others easily). \n",
    "\n",
    "## Preparing the work\n",
    "\n",
    "Let's initialize the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to cohere, navigate to DASHBOARD (at the top), then to API Keys (at the sidebar)\n",
    "\n",
    "# Never store your key in the code or push it to your repository!!!\n",
    "\n",
    "with open(\"cohere.key\") as f:\n",
    "    COHERE_API_KEY = f.read()\n",
    "cohere_client = cohere.ClientV2(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 1: Prompt templates\n",
    "\n",
    "### Prompt templates from scratch\n",
    "\n",
    "LLM APIs allow us to incorporate conversational interactions in our applications to replace or complement traditional code. \n",
    "\n",
    "For example, we can call a LLM to classify the language of a text, instead of writing the code of a ML language classifier. Or we can ask an LLM to extract relevant fields from a text, instead of using a form with predefined fields.\n",
    "\n",
    "The basic mechanism to do this is to create a prompt template, which is a structured way of asking the LLM for a specific task. In this notebook, we will learn patterns to create prompt templates, and we will see how to use them in different use cases.\n",
    "\n",
    "For example, let's say we want to classify the language of a text. Let's use a very simple approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language of the text between triple quotation marks is **Catalan**.\n"
     ]
    }
   ],
   "source": [
    "simple_prompt_template = \"Return the language of the text between triple quotation marks: ```!text!``` \" \n",
    "\n",
    "user_text = input(\"Enter some text\")\n",
    "final_prompt = simple_prompt_template.replace(\"!text!\", user_text)\n",
    "\n",
    "formatted_message = {\n",
    "    'role': 'user',\n",
    "    'content': final_prompt \n",
    "}\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    model=\"command-a-03-2025\",\n",
    "    messages=[formatted_message],\n",
    ")\n",
    "\n",
    "language = response.message.content[0].text\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6b1dee43-80fa-45d5-bf95-8ac4f68b8229',\n",
       " 'finish_reason': 'COMPLETE',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'The language of the text between triple quotation marks is **Catalan**.'}]},\n",
       " 'usage': {'billed_units': {'input_tokens': 21.0, 'output_tokens': 14.0},\n",
       "  'tokens': {'input_tokens': 517.0, 'output_tokens': 17.0},\n",
       "  'cached_tokens': 448.0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good but has a few problems. The LLM will try to be helpful and give us a long answer, instead of just the name of the language. Also, you might need to specify additional instructions, such as the list of languages to choose from, or what to do if the language is not in the list.\n",
    "\n",
    "A common, well-established pattern to create prompt templates is the following:\n",
    "\n",
    "1. Define the task and the input. \n",
    "2. Define the output format. \n",
    "3. Provide examples.\n",
    "4. Add any additional instructions or constraints. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CATALAN'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text = input(\"Enter some text\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "You are a language identification system.\n",
    "\n",
    "Task:\n",
    "Detect the primary natural language of the user-provided text.\n",
    "\n",
    "Instructions:\n",
    "- The user input will appear between triple backticks.\n",
    "- Identify the dominant language of the text.\n",
    "- Respond with exactly ONE word in UPPERCASE.\n",
    "- Do NOT include punctuation, explanations, or additional text.\n",
    "- If the language cannot be determined confidently, respond with: UNKNOWN\n",
    "- If the text contains multiple languages, respond with the dominant one.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "```Hello, how are you?```\n",
    "Output:\n",
    "ENGLISH\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "Input:\n",
    "```{user_text}```\n",
    "Output:\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    messages = messages,\n",
    "    model='command-a-03-2025',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "response.message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: In-prompt examples\n",
    "\n",
    "We can use LLMs to \"simulate\" the way AI systems work and provide \"examples\" of what we want to achieve, as part of the prompt. \n",
    "\n",
    "This is know as \"few-shot\", \"in-prompt\" learning. \n",
    "\n",
    "The example below tries to classify the restaurant type based on its name. It does so by preparing a \"template\" of known inputs and outputs, then asking to complete a last input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = input(\"Enter some text\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "You are a system for customer support in banking.\n",
    "Your first task is to detect the part of the request that refers to all details \n",
    "of a specific transaction and add the tags [TRANSACTION] and [/TRANSACTION] around it.\n",
    "Detais include: date, amount, location, and merchant. \n",
    "The user input will appear between triple backticks.\n",
    "If there is no transaction, just return the original text without any tags.\n",
    "Examples: \n",
    "Input: ```Hello, good morning. I have a problem with a payment of 5.60 EUR made in Starbucks yesterday. Thanks!```\n",
    "Segmentation: Hello, good morning. I have a problem with a payment of [TRANSACTION]5.60 EUR made in Starbucks yesterday[/TRANSACTION]. Thanks!\n",
    "Input: ```What is this??? I don't recognize a payment of 1200 EUR in Amazon on the 5th of May.```\n",
    "Segmentation: What is this??? I don't recognize a payment of [TRANSACTION]1200 EUR in Amazon on the 5th of May[/TRANSACTION].\n",
    "Input: ```Hi I want information on mortgage loans. Also, I want to cancel a direct debit of 50 EUR in the gym, I don't have time to go there!```\n",
    "Segmentation: Hi I want information on mortgage loans. Also, I want to cancel a [TRANSACTION]direct debit of 50 EUR in the gym[/TRANSACTION], I don't have time to go there!\n",
    "Input: ```I want to know the balance of my account.```\n",
    "Segmentation: I want to know the balance of my account.\n",
    "Input: ```How much is the comission for a new credit card?```\n",
    "Segmentation: How much is the comission for a new credit card?\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Input:\n",
    "```{user_text}```\n",
    "Segmentation:\n",
    "\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, I am a customer of yours. The insurance Zurich charged [TRANSACTION]100 euros more than expected. Direct debit with data 23/2/2026[/TRANSACTION]. Can you help me with this?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = cohere_client.chat(\n",
    "    messages = messages,\n",
    "    model='command-a-03-2025',\n",
    "    temperature=0.0,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "response.message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the cost increases with the number of examples. You can get info of the costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '09a13233-003e-4ba2-b8f2-c783b66822c8',\n",
       " 'finish_reason': 'COMPLETE',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Hi, I am a customer of yours. The insurance Zurich charged [TRANSACTION]100 euros more than expected. Direct debit with data 23/2/2026[/TRANSACTION]. Can you help me with this?'}]},\n",
       " 'usage': {'billed_units': {'input_tokens': 427.0, 'output_tokens': 48.0},\n",
       "  'tokens': {'input_tokens': 956.0, 'output_tokens': 51.0},\n",
       "  'cached_tokens': 480.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: LLMs as user interface\n",
    "\n",
    "Natural language can be the new interface. \n",
    "\n",
    "The example below asks the user to write a text specifying whether to turn the lights on or off.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b80e00ff0a4d44aecd8111558efabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Lights on', indent=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Lights on',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Just for debugging, the output text is true )\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter your message\")\n",
    "response = cohere_client.chat(\n",
    "    messages = [{\n",
    "        'role': 'system',\n",
    "        'content': \"\"\"\n",
    "The user will write a text about turning off or on the lights.\n",
    "The user might use different words to describe the action or expressions, \n",
    "like turn on, turn off, activate, deactivate, switch on, switch off, etc.\n",
    "The user might also use different words to describe the lights,\n",
    "like lights, lamps, bulbs, etc.\n",
    "Return a single word: \"true\" if the user wants to turn on the lights, and \"false\" if the user wants to turn them off.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': user_input\n",
    "    }\n",
    "    ],\n",
    "    model = 'command-a-03-2025',\n",
    ")\n",
    "\n",
    "outcome = response.message.content[0].text\n",
    "print(\"(Just for debugging, the output text is\", outcome, \")\")\n",
    "\n",
    "if outcome == \"true\":\n",
    "    checkbox.value = True\n",
    "elif outcome == \"false\":\n",
    "    checkbox.value = False\n",
    "else:\n",
    "    print(\"Unexpected outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: Structuring data\n",
    "\n",
    "Use LLMs to extract information and add it to a certain template. \n",
    "\n",
    "The next example simulates a text from a health record and how the system would extract the necessary information into a template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"age\": \"Not found\",\n",
      "  \"temperature\": 28,\n",
      "  \"reason for visit\": \"Not found\",\n",
      "  \"tests performed\": [\"blood test\"],\n",
      "  \"diagnosis\": \"Not found\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "The user will write a text simulating an electronic health record.\n",
    "Extract the information necessary to populate this json object:\n",
    "{\n",
    "  \"age\": X,\n",
    "  \"temperature\": X,\n",
    "  \"reason for visit\": \"X\",\n",
    "  \"tests performed\": [\"X\", \"X\", \"X\"],\n",
    "    \"diagnosis\": \"X\"\n",
    "}\n",
    "If the information is not present, return \"Not found\".\n",
    "Respond only with the json object. \n",
    "\"\"\"\n",
    "\n",
    "user_input = input(\"Enter your message\")\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    messages = [\n",
    "      {\n",
    "        'role': 'system',\n",
    "        'content': system_prompt  \n",
    "      }, \n",
    "      {\n",
    "          'role': 'user',\n",
    "          'content': user_input\n",
    "      }\n",
    "    ],\n",
    "    model='command-a-03-2025',\n",
    ")\n",
    "\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bear in mind: chat history\n",
    "\n",
    "This example illustrates the use of a chatbot, where the intent to be fulfilled is accomplished after several messages, and each message needs to include the previous chat history. \n",
    "\n",
    "Let's do it in the wrong way first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have enough information to answer your question. Could you please provide the name of the person you are referring to? I'll be happy to help you find the birth date if you give me the necessary details.\n"
     ]
    }
   ],
   "source": [
    "message1 = {'role': 'user', 'content': 'Who discovered polonium and radium?'}\n",
    "message2 = {'role': 'user', 'content': 'When was she born?'}\n",
    "\n",
    "response1 = cohere_client.chat(messages=[message1], model='command-a-03-2025')\n",
    "response2 = cohere_client.chat(messages=[message2], model='command-a-03-2025')\n",
    "\n",
    "print(response2.message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text=\"Polonium and radium were discovered by **Marie Curie** and **Pierre Curie**, a husband-and-wife team of scientists, in the late 19th century. Specifically:\\n\\n1. **Polonium** (element 84) was discovered in 1898 by Marie and Pierre Curie. They named it after Marie's homeland, Poland (Latin: *Polonia*).\\n\\n2. **Radium** (element 88) was also discovered by Marie and Pierre Curie in 1898, while they were studying pitchblende, a uranium-rich mineral. They isolated radium and recognized its highly radioactive properties.\\n\\nMarie Curie's work on radioactivity, including the discovery of polonium and radium, earned her the Nobel Prize in Physics in 1903 (shared with Pierre Curie and Henri Becquerel) and the Nobel Prize in Chemistry in 1911.\")], citations=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add the previous chat history, that was stored already in the response json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie was born on **November 7, 1867**, in Warsaw, Poland (then part of the Russian Empire). She is widely regarded as one of the most influential scientists in history, known for her pioneering work on radioactivity and her discoveries of polonium and radium.\n"
     ]
    }
   ],
   "source": [
    "response = cohere_client.chat(\n",
    "    messages=[\n",
    "        message1,\n",
    "        response1.message,\n",
    "        message2\n",
    "    ],\n",
    "    model='command-a-03-2025',\n",
    ")\n",
    "print(response.message.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text=\"Polonium and radium were discovered by **Marie Curie** and **Pierre Curie**, a husband-and-wife team of scientists, in the late 19th century. Specifically:\\n\\n1. **Polonium** (element 84) was discovered in 1898 by Marie and Pierre Curie. They named it after Marie's homeland, Poland (Latin: *Polonia*).\\n\\n2. **Radium** (element 88) was also discovered by Marie and Pierre Curie in 1898, while they were studying pitchblende, a uranium-rich mineral. They isolated radium and recognized its highly radioactive properties.\\n\\nMarie Curie's work on radioactivity, including the discovery of polonium and radium, earned her the Nobel Prize in Physics in 1903 (shared with Pierre Curie and Henri Becquerel) and the Nobel Prize in Chemistry in 1911.\")], citations=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1.message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
